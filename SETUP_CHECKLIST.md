# ğŸš€ Setup Checklist

Follow this checklist to get your Ollama-powered chatbot running.

## âœ… Prerequisites

- [ ] Node.js installed (v18 or higher recommended)
- [ ] npm or yarn package manager
- [ ] Ollama installed on your system

## ğŸ“‹ Step-by-Step Setup

### 1. Install Ollama
```bash
# macOS
brew install ollama

# Or download from https://ollama.ai
```

- [ ] Ollama installed successfully

### 2. Pull the Model
```bash
ollama pull llama3.1:8b
```

- [ ] Model downloaded (this may take several minutes depending on your connection)

### 3. Start Ollama Server
```bash
ollama serve
```

- [ ] Ollama server running on http://localhost:11434
- [ ] Leave this terminal window open

### 4. Install Project Dependencies
```bash
npm install
```

- [ ] All npm packages installed successfully

### 5. Test Ollama Connection
```bash
npm run test:ollama
```

Expected output:
```
âœ… Ollama is running
âœ… Model is installed
âœ… Generation successful
```

- [ ] All tests passed

### 6. Start the Application
```bash
npm start
```

This will start:
- Frontend on http://localhost:5173
- Backend on http://localhost:3001

- [ ] Both servers started successfully
- [ ] No error messages in console

### 7. Test in Browser

1. Open http://localhost:5173
2. Click "Get Started"
3. Type a message and press Send
4. Verify you receive a streaming response from the AI

- [ ] Frontend loads successfully
- [ ] Can send messages
- [ ] Receive AI responses
- [ ] Streaming works (tokens appear gradually)

## ğŸ” Verification Commands

Run these to verify each component:

```bash
# Check Ollama
curl http://localhost:11434/api/version

# Check Backend
curl http://localhost:3001/health

# Check Frontend
# Open browser to http://localhost:5173
```

## ğŸ› Common Issues

### Issue: "Ollama server not responding"
**Solution:**
```bash
# Start Ollama in a separate terminal
ollama serve
```

### Issue: "Model not found"
**Solution:**
```bash
ollama pull llama3.1:8b
```

### Issue: "Port 3001 already in use"
**Solution:**
```bash
# Find and kill the process
lsof -ti:3001 | xargs kill -9

# Or change port in server.js
```

### Issue: "CORS error in browser"
**Solution:**
- Backend should already have CORS enabled
- Make sure backend is running on port 3001
- Check browser console for exact error

### Issue: "Failed to fetch"
**Solution:**
```bash
# Make sure backend is running
npm run server

# In another terminal, test it
curl http://localhost:3001/health
```

## ğŸ“ File Structure

```
Lavender/
â”œâ”€â”€ server.js              # Express backend with Ollama integration
â”œâ”€â”€ test-ollama.js         # Ollama connection test script
â”œâ”€â”€ README_OLLAMA.md       # Detailed documentation
â”œâ”€â”€ API_REFERENCE.md       # API quick reference
â”œâ”€â”€ package.json           # Dependencies and scripts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx           # Frontend with streaming support
â”‚   â”œâ”€â”€ main.jsx          # React entry point
â”‚   â””â”€â”€ style.css         # Styles
â””â”€â”€ public/               # Static assets
```

## ğŸ¯ Quick Commands Reference

| Command | Description |
|---------|-------------|
| `npm start` | Start frontend + backend |
| `npm run dev` | Start frontend only |
| `npm run server` | Start backend only |
| `npm run test:ollama` | Test Ollama connection |
| `ollama serve` | Start Ollama server |
| `ollama list` | List installed models |

## âœ¨ You're All Set!

Once all checkboxes are ticked, you have:
- âœ… Ollama running locally
- âœ… Backend API serving on port 3001
- âœ… Frontend UI on port 5173
- âœ… Streaming chat responses working

## ğŸ¨ Customization

Want to customize? Check these files:

- **Change model:** Edit `MODEL_NAME` in [server.js](server.js#L7)
- **Change port:** Edit `PORT` in [server.js](server.js#L6)
- **Styling:** Edit [src/style.css](src/style.css)
- **UI/UX:** Edit [src/App.jsx](src/App.jsx)

## ğŸ“š Next Steps

1. **Try different models:**
   ```bash
   ollama pull mistral
   ollama pull phi3
   ollama pull codellama
   ```

2. **Add features:**
   - Message history
   - Model selection dropdown
   - Temperature/parameter controls
   - Save conversations

3. **Deploy:**
   - See [README_OLLAMA.md](README_OLLAMA.md) for production notes
   - Consider rate limiting and authentication

## ğŸ’¬ Need Help?

1. Check logs in terminal
2. Run `npm run test:ollama`
3. Review [README_OLLAMA.md](README_OLLAMA.md)
4. Check [API_REFERENCE.md](API_REFERENCE.md)

---

**Happy Coding! ğŸ‰**
